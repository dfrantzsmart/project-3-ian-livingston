{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Set-up\n",
    "In this notebook I've provided the primary code I used to initialize and tune my model. I attempted to trim some of the inefficiency in my first solo project by coding my models into pipeline functions, and as a result much of my modeling work can be seen in the nba_modeling_functions.py file in this repo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, classification_report, roc_auc_score, roc_curve, confusion_matrix, auc, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost as xgb\n",
    "import nba_all_modeling_functions as nbam\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 600)\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# Unpickling my dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"supdated_df.pickle\", \"rb\") as read_file:\n",
    "    all_data_df = pickle.load(read_file)"
   ]
  },
  {
   "source": [
    "# Initial attempt at model (logistic regression)\n",
    "Initially I threw three simple features into a logistic regression model. I got the following results:\n",
    "\n",
    "- Precision (non-All-Stars): 0.98\n",
    "- Precision (all-Stars): 0.80\n",
    "- Precision (weighted average): 0.97\n",
    "- Recall (non-All-Stars): 0.99\n",
    "- Recall (All-Stars): 0.59\n",
    "- Recall (weighted average): 0.97\n",
    "- F1 score (non-All-Stars): 0.99\n",
    "- F1 score (All-Stars): 0.68\n",
    "\n",
    "I also printed out a classification report and plotted and confusion matrix. The initial results above were tough to improve upon throughout."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using features I suspected would have high correlation\n",
    "features = [\"PTS\", \"AST\", \"PER\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "# Scaling training set (for now) and testing set (for later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# The prob predictions I looked at separately\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train_scaled, y_train)\n",
    "logr_val_predictions = logr.predict(X_val_scaled)\n",
    "logr_prob_predictions = logr.predict_proba(X_val_scaled)\n",
    "logr_prob_dict = dict(zip(list(X_val.index), list(logr_prob_predictions)))\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, logr_val_predictions)}')\n",
    "print(classification_report(y_val, logr_val_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "plot_confusion_matrix(logr, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\")"
   ]
  },
  {
   "source": [
    "# Next baseline model: KNN (k of arbitrarily chosen 5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "knn_predictions = nn.predict(X_val_scaled)\n",
    "knn_prob_predictions = nn.predict_proba(X_val_scaled)\n",
    "knn_tn, knn_fp, knn_fn, knn_tp = confusion_matrix(y_val, knn_predictions).ravel()\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, knn_predictions)}')\n",
    "print(classification_report(y_val, knn_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(nn, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "source": [
    "# Plotting the above two baseline models on an ROC curve\n",
    "Early on, logistic regression looked to be a better option than logistice regression. That said I knew I still needed to try more models, add or at least try a ton more features, tune my k hyperparameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logr_pos_preds = logr_prob_predictions[:, 1]\n",
    "logr_fpr, logr_tpr, logr_threshold = roc_curve(y_val, logr_pos_preds)\n",
    "roc_auc = auc(logr_fpr, logr_tpr)\n",
    "\n",
    "# KNN model\n",
    "knn_pos_preds = knn_prob_predictions[:, 1]\n",
    "knn_fpr, knn_tpr, knn_threshold = roc_curve(y_val, knn_pos_preds)\n",
    "roc_auc = auc(knn_fpr, knn_tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7), sharex=True, sharey=True)\n",
    "ax.set_title('ROC', fontdict={\"fontsize\": 25}, y=1.05)\n",
    "ax.plot(logr_fpr, logr_tpr, label=\"Logistic Regression\", lw=4)\n",
    "ax.plot(knn_fpr, knn_tpr, label=\"KNN (k=5)\", lw=4)\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.plot([0, 1], [0, 1],'r--');"
   ]
  },
  {
   "source": [
    "# Second round of models\n",
    "Before I wizened up and built my pipeline functions I incrementally built upon these baseline models by deliberately adding features. I also tried out some different k values. Instead of flooding this notebook with all that I've scratched out a select sample below, including my initial attempts at fitting random forest, Naive Bayes, and SVC (with SMOTE) models to my dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Second attempt at KNN (adding \"MP\" and \"Age\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PTS\", \"AST\", \"PER\", \"MP\", \"Age\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "# Scaling training set (for now) and validation set (for later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Instantiating and fitting\n",
    "nn = KNeighborsClassifier(n_neighbors=20, n_jobs=-1)\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "knn_predictions = nn.predict(X_val_scaled)\n",
    "knn_prob_predictions = nn.predict_proba(X_val_scaled)\n",
    "knn_tn, knn_fp, knn_fn, knn_tp = confusion_matrix(y_val, knn_predictions).ravel()\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, knn_predictions)}')\n",
    "print(classification_report(y_val, knn_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(nn, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "source": [
    "## Another attempt at logistic regression...\n",
    "...with the same features as above plus \"All-Star?\" which is All-Star status the season prior to that which is the target year)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PTS\", \"AST\", \"PER\", \"MP\", \"Age\", \"All-Star?\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "# Scaling training set (for now) and validation set (for later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Instantiating and fitting\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train_scaled, y_train)\n",
    "logr_val_predictions = logr.predict(X_val_scaled)\n",
    "logr_tn, logr_fp, logr_fn, logr_tp = confusion_matrix(y_val, logr_val_predictions).ravel()\n",
    "logr_prob_predictions = logr.predict_proba(X_val_scaled)\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, logr_val_predictions)}')\n",
    "print(classification_report(y_val, predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(logr, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "source": [
    "## First random forest model...\n",
    "...with the same features as above. My training and validation sets remained scaled, even though they did not need to be. I simply pumped the material from the last logistic regression model right in here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_for = RandomForestClassifier()\n",
    "ran_for.fit(X_train_scaled, y_train)\n",
    "ran_for_predictions = ran_for.predict(X_val)\n",
    "ran_for_prob_predictions = ran_for.predict_proba(X_val_scaled)\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, ran_for_predictions)}')\n",
    "print(classification_report(y_val, ran_for_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(ran_for, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a look at feature importance as determined by this random forest model\n",
    "print(ran_for.feature_importances_)"
   ]
  },
  {
   "source": [
    "## First Naive Bayes model...\n",
    "...also with the same features as above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes (Gaussian)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "gnb_predictions = gnb.predict(X_val)\n",
    "gnb_prob_predictions = gnb.predict_proba(X_val_scaled)\n",
    "gnb_tn, gnb_fp, gnb_fn, gnb_tp = confusion_matrix(y_val, gnb_predictions).ravel()\n",
    "\n",
    "# Performance\n",
    "print(classification_report(y_val, gnb_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(gnb, X_val, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "source": [
    "# Another ROC curve...\n",
    "...for another look at all the models so far against one another. Logistic regression at this point no longer lapped the field but still seemed the likely best fit among the group, both in terms of performance so far, AUC showing, and the interpretability it offers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logr_pos_preds = logr_prob_predictions[:, 1]\n",
    "logr_fpr, logr_tpr, logr_threshold = roc_curve(y_val, logr_pos_preds)\n",
    "logr_roc_auc = auc(logr_fpr, logr_tpr)\n",
    "\n",
    "# KNN\n",
    "knn_pos_preds = knn_prob_predictions[:, 1]\n",
    "knn_fpr, knn_tpr, knn_threshold = roc_curve(y_val, knn_pos_preds)\n",
    "knn_roc_auc = auc(knn_fpr, knn_tpr)\n",
    "\n",
    "# Random forest\n",
    "ran_for_pos_preds = ran_for_prob_predictions[:, 1]\n",
    "ran_for_fpr, ran_for_tpr, ran_for_threshold = roc_curve(y_val, ran_for_pos_preds)\n",
    "ran_for_roc_auc = auc(ran_for_fpr, ran_for_tpr)\n",
    "\n",
    "# Naive Bayes\n",
    "gnb_pos_preds = gnb_prob_predictions[:, 1]\n",
    "gnb_fpr, gnb_tpr, gnb_threshold = roc_curve(y_val, gnb_pos_preds)\n",
    "gnb_roc_auc = auc(gnb_fpr, gnb_tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7), sharex=True, sharey=True)\n",
    "ax.set_title('ROC', fontdict={\"fontsize\": 25}, y=1.05)\n",
    "ax.plot(logr_fpr, logr_tpr, label=\"Logistic Regression\", lw=4)\n",
    "ax.plot(knn_fpr, knn_tpr, label=\"KNN (k=5)\", lw=4)\n",
    "ax.plot(ran_for_fpr, ran_for_tpr, label=\"Random forest\", lw=4)\n",
    "ax.plot(gnb_fpr, gnb_tpr, label=\"Naive Bayes (Gaussian)\", lw=4)\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.plot([0, 1], [0, 1],'r--');"
   ]
  },
  {
   "source": [
    "# In optimizing my models, I identified the following values as best-fit hyperparameters:\n",
    " \n",
    " - k: 9 (KNN)\n",
    " - max_depth: 5 (random forest)\n",
    " - min_samples_split: 7 (random forest)\n",
    "\n",
    "Below are the processes I used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Finding best k"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Default best k value (sqrt): {np.sqrt(all_data_df.shape[0]}')\n",
    "\n",
    "features = [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\", \"TRB/game\", \"PTS+AST/game\", \"MP/game\", \"FT/game\", \"All-Star next season?\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "acc = []\n",
    "for k in range(1,30):\n",
    "    knn_ks = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)\n",
    "    predictions_ks = knn_ks.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, predictions_ks))\n",
    "    \n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(range(1, 30), acc, color = \"cornflowerblue\", marker='o', markerfacecolor=\"cornflowerblue\", markersize=10)\n",
    "plt.title(\"K value x accuracy\", fontdict={\"fontsize\":20}, y=1.05)\n",
    "plt.xlabel(\"K\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16, rotation=90)\n",
    "print(f'Best k: {acc.index(max(acc))} with accuracy score of {max(acc)}');"
   ]
  },
  {
   "source": [
    "## Finding best max_depth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\", \"TRB/game\", \"PTS+AST/game\", \"MP/game\", \"FT/game\", \"0\", \"1-3\", \"4-7\", \"8+\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "acc = []\n",
    "for depth in range(1,30):\n",
    "    ran_for = RandomForestClassifier(max_depth=depth).fit(X_train, y_train)\n",
    "    ran_for_predictions = ran_for.predict(X_val)\n",
    "    acc.append(accuracy_score(y_val, ran_for_predictions))\n",
    "    \n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(range(1, 30), acc, color = \"cornflowerblue\", marker='o', markerfacecolor=\"cornflowerblue\", markersize=10)\n",
    "plt.title(\"max_depth x Accuracy\", fontdict={\"fontsize\":20}, y=1.05)\n",
    "plt.xlabel(\"max_depth\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16, rotation=90)\n",
    "print(f'Best max_depth: {acc.index(max(acc))} with accuracy score of {max(acc)}');\n",
    "plt.savefig(\"Best max_depth value.svg\");"
   ]
  },
  {
   "source": [
    "## Finding best min_samples_split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\", \"TRB/game\", \"PTS+AST/game\", \"MP/game\", \"FT/game\", \"0\", \"1-3\", \"4-7\", \"8+\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "acc = []\n",
    "for depth in range(1,30):\n",
    "    ran_for = RandomForestClassifier(max_depth=depth).fit(X_train, y_train)\n",
    "    ran_for_predictions = ran_for.predict(X_val)\n",
    "    acc.append(accuracy_score(y_val, ran_for_predictions))\n",
    "    \n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(range(1, 30), acc, color = \"cornflowerblue\", marker='o', markerfacecolor=\"cornflowerblue\", markersize=10)\n",
    "plt.title(\"max_depth x Accuracy\", fontdict={\"fontsize\":20}, y=1.05)\n",
    "plt.xlabel(\"max_depth\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16, rotation=90)\n",
    "print(f'Best max_depth: {acc.index(max(acc))} with accuracy score of {max(acc)}');\n",
    "plt.savefig(\"Best max_depth value.svg\");"
   ]
  },
  {
   "source": [
    "# More model tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## After engineering per game stats from the totals\n",
    "\"TV market size,\" which I added in a previous step, didn't do anything at all on its own. Disappointing but not surprising, given how reductive it was as a measure of player profile/star power. The per-game-adjusted features meanwhile didn't brought me a .01 bump in my precision  but didn't return the sort of leap I thought was possible. One thought I had here is that the NBA's top tier in key measures really do sit far from the mean. I engineered \"...relative\" version of these measures in an attempt to emphasize the strength of the best performers but that didn't get me anywhere new. I'm pretty sure I just normalized the data as the likes of a StandardScaler does."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PTS/game\", \"AST/game\", \"PER\", \"MP/game\", \"Age\", \"All-Star?\", \"TV market size\"]\n",
    "X = all_data_df[features]\n",
    "y = all_data_df[\"All-Star next season?\"]\n",
    "\n",
    "# Test-train split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=10)\n",
    "\n",
    "# Scaling training set (for now) and validation set (for later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Instantiating and fitting\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train_scaled, y_train)\n",
    "logr_val_predictions = logr.predict(X_val_scaled)\n",
    "logr_tn, logr_fp, logr_fn, logr_tp = confusion_matrix(y_val, logr_val_predictions).ravel()\n",
    "logr_prob_predictions = logr.predict_proba(X_val_scaled)\n",
    "\n",
    "# Performance\n",
    "print(f'Accuracy (val): {accuracy_score(y_val, logr_val_predictions)}')\n",
    "print(classification_report(y_val, logr_val_predictions))\n",
    "fig, ax = plt.subplots(figsize=(7, 7));\n",
    "plot_confusion_matrix(logr, X_val_scaled, y_val, ax=ax, cmap=\"Oranges\");"
   ]
  },
  {
   "source": [
    "# Modeling with pipeline functions\n",
    "At this point, or some point, I moved the work I was simply repeating and opening to unnecessary error into a handful of functions that included train_test splits (random_seed=10), scaling, training/fitting, cross-validation, scoring (including log loss), and plotting confusion matrices. I used these functions to inefficiently toggle on/off combinations of my dataset's 50+ features, including features I went back to scraping to obtain. I iterated through combinations, favoring logistic regression, until I pulled four models into an ensemble. I also gave SVC and SMOTE a shot. Not much luck with that. Among my other observations:\n",
    "\n",
    "- \"BLK/game\" and \"STL/game\" didn't help, in line with the perception that defense doesn't make an All-Star\n",
    "- \"WS/48\" slightly improved precision but knocksed recall and overall score\n",
    "- \"TS%\" didn't help (volume over efficiency?)\n",
    "- My \"Years from prime\" feature, which I thought might better quantify age relative to accepted average peak (27), and then my \"Years from prime ^ 2\" feature to give it extra weight, didn't return huge gains\n",
    "- \"Trajectory\" added the most to my overall accuracy of all of my engineered features\n",
    "- Though I built them into my pipeline functions, neither RandomOverSampler nor RandomUnderSampler had any tangible effect for reasons I just do not see clearly (even plugging in different values for sampling_strategy)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## First SV model...\n",
    "...in part to use SMOTE as a means of smoothing out irregularities caused by my imbalanced targets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, class_report, con_matrix = nba_svc(all_data_df, [\"PTS/game relative\", \"AST/game relative\", \"PER\", \"MP/game\", \"Age\", \"All-Star?\"], \"All-Star next season?\", SMOTE=True, print_all=True)"
   ]
  },
  {
   "source": [
    "## Logistic regression function in action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, logloss, class_report, con_matrix = nba_log_regression(all_data_df, [\"PTS/game\", \"AST/game\", \"PER\", \"MP/game\", \"Age\", \"Adjusted TV market value\", \"All-Star?\"], \"All-Star next season?\", print_all=True)"
   ]
  },
  {
   "source": [
    "## KNN function in action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, logloss, class_report, con_matrix = nba_knn(all_data_df, [\"All-Star?\", \"PTS/game\", \"AST/game\", \"PER\", \"MP/game\", \"Years from prime\", \"Trajectory\", \"Adjusted TV market value * GS\"], \"All-Star next season?\", RandomOverSampler=True, print_all=True)"
   ]
  },
  {
   "source": [
    "## Random forest function in action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, logloss, class_report, con_matrix = nba_random_forest(all_data_df, [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\"], \"All-Star next season?\", RandomOverSampler=True, print_all=True)"
   ]
  },
  {
   "source": [
    "## SVC function in action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, logloss, class_report, con_matrix = nba_svc(all_data_df, [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\"], \"All-Star next season?\", print_all=True)"
   ]
  },
  {
   "source": [
    "# Ensembling and predictions\n",
    "My feature engineering, modeling and hyperparameter tuning didn't return a single model that could predict the exact set of NBA All-Stars in a given season, though it did get 23/24 on the 2015-16 season I held out at the beginning. A very good result, but still out of sync with the results I was getting again and again on my training sets. My last effort was to try XGBoost, which surprisingly didn't improve my score at all, and a VotingClassifier() ensemble, which did, though marginally. My final model was an ensemble containing a logistic regression model, a KNN model (K=9), a random forest model (max_depth=5, min_samples_split=7), and an SVC model. It performed near-negligably better than my initial, three-feature model, proving I guess that you need more than two weeks to innovate as an NBA data trader. And so I will try again...\n",
    "\n",
    "In the meantime, I put together a function that used my final model in the manner I made my goal two weeks back: as a means of predicting 24 players exactly when fed a single season worth of data—the 24 players projected as most likely to be selected as All-Stars in the following season. That function which I built into my first Flask app (currently only hosted locally), is below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbam.nba_ensemble_predict(all_data_df, [\"All-Star?\", \"PTS/game\", \"AST/game\", \"Years from prime\", \"PER\", \"Trajectory\", \"Adjusted TV market value * GS\", \"TRB/game\", \"PTS+AST/game\", \"MP/game\", \"FT/game\"], \"All-Star next season?\", plot=False)"
   ]
  }
 ]
}